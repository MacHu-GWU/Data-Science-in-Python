Bagging
==============================================================================
Bagging (集成) 算法的核心思路:

1. 从样本中重采样(有重复的)选出N个样本 (bootstrap策略).
2. 在所有属性上, 对这N个样本建立分类器. (Decision Tree, SVM, Logistic Regression)
3. 重复以上两步m次, 获得了m个分类器.
4. 根据这m个分类器进行投票, 决定数据属于哪一类.

代表算法: 随机森林. 我们用随机森林为例, 来解释集成中我们会遇到的问题.


Bootstrap策略
------------------------------------------------------------------------------


1. 如果有重复的选出N个样本, 里面平均有百分之多少的样本会被选中?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
::

    P(被选中的概率) = 1 - ( 1 - 1/N) ** N
    N -> 无穷大 lim P = 1 - 1/e = 63.2%

可以推断出, 平均有63.2%的数据会被选中 (bootstrap sample), 36.8%的数据不会被选中, 称为袋外数据(Out of bag)

这些袋外数据可以用来做误差估计 (Evaluate Bias), 判断当前的模型是否可用. 即用OOB数据和测试数据来衡量, 效果差不多.


2. 我们一定要选N个样本吗? 可以选 N/2 个么?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
其实是可以的, 于是, 我们就得到了一个超参数, 采样率 ``α``, 决定了我们每次从中随机选取 ``α * N`` 个样本.



特征选择
------------------------------------------------------------------------------
- 标准做法: 当我们选取了一个根节点之后, 我们要在其他所有特征进行测试, 看看以哪个特征进行分类, 能使得信息增益最大.
- 随机森林做法: 我们不一定要对所有的特征进行测试,


投票机制
------------------------------------------------------------------------------
- 简单投票机制
    - 一票否决制 (一致表决)
    - 少数服从多数
        - 有效多数 (加权)
    - 阈值表决
- 贝叶斯投票机制

考虑一个例子: IMDB电影评分网站, 用户对电影进行评分, 对于冷门电影, 可能参与投票的用户会比较少, 此时这些少数用户的票可能并不能反映出真实的评分. 所以我们考虑下面的评分计算机制, 对这种情况做平滑.

加权平滑评分: ``WR (Weighted Rate) = (v / (v + m)) * R + (m / (v + m)) * C``

- WR: 加权得分.
- R: 该电影的平均分.
- C: 所有电影的平均分.
- v: 该电影的投票人数.
- m: 超参数, m是一个我们自己约定的参数, 可以用来决定平滑的力度. 例如, 排名前250名的电影的最低投票数.

根据评分定义式, 当v很少时, 用户评分占的权值就比较小, 而所有电影的平均分就会将用户的平均分拉得向所有电影平均分一点. 而当v很多时, 就能可以真实的反映出电影的评分了.



总结: Bagging算法相关的参数
------------------------------------------------------------------------------



哪些算法比较适合Bagging的策略?
------------------------------------------------------------------------------
我们通常并不会用强分类器算法和Bagging策略进行结合.
