Kmean (K均值)
==============================================================================

- 对初始值敏感


如何选择初始值
------------------------------------------------------------------------------

Kmean ++

1.


如何选择样本数量K
------------------------------------------------------------------------------
我们可以一开始选择一个小的K, 当收敛之后, 对样本数较多的类做分层Kmean, 也就是在类内再进行Kmean


簇中心变化率
最小平方误差



解决初值敏感的方法:

选择初值:

KMean++算法
------------------------------------------------------------------------------
**KMean++解决的是 "选取几个比较可靠的初始值的" 问题**.

我们希望一开始选中的几个初值互相之间距离比较远. 所以我们可以这样做:

1. 首先随便选一个点a1.
2. 然后我们选第二个点的时候, 尽量选离第一个远的点, 具体做法是:
    - 计算所有其他点到a1的距离, 有d1, d2, ..., dn-1, 然后除以所有距离之和, 那么就得到了p1, p2, ..., pn-1.
    - 我们把p当做是概率, 依据概率选取a2, 这样可以使得距离远的点被选中的概率大, 距离近的点被选择的概率小.
3. 当我们有了a1, a2, 选a3时, 我们再计算所有点对a2的距离, 如果这个距离比离a1大, 那么就更新一下. 这样我们更新完所有的距离之后, 仍然可以依概率随机选择a3.
4. 不断重复, 直到选择了k个初始点为止.


K-medoids算法
------------------------------------------------------------------------------
在K均值算法中, 我们每次迭代之后, 是选取每个类中所有点的平均值作为该类的中心. 而在K-medoids算法中, 是在类内选取一个特殊的点, 这个点离类内其他点的距离之和最小.

优点:

对于类标数据, 我们常常使用HotEncoding, 而我们无法对类标数据也计算一个均值. 而K-medoids就可以.

缺点:

在计算中心点时, 复杂度一下子就从O(n)提高到了O(n2).