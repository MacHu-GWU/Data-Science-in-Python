Decision Tree (决策树)
==============================================================================
在理解了熵(Entropy)的概念之后, 我们可以这样描述决策树.

ID3: 信息增益, ``max -> H(X) - H(X|fi)``. (fi is ith feature)

C4.5: 信息增益率, ``max -> (H(X) - H(X|fi)) / H(fi)``

Cart: 基尼系数(Gini), `` sigma k = 1 ~ K : pk * ( 1 - pk)``

我们最常用的是基尼系数.

为什么Gini系数能够

记得熵的公式么? ``H(X) = - sigma: P(xi) * log(P(xi))``, 基尼系数可以理解为就是熵的定义式, 只不过让 ``- ln(x) ~= (1-x)`` 在0点处泰勒展开, 取一阶项.


决策树(DT)的过拟合
------------------------------------------------------------------------------

1. 剪枝 (Trim, Pre Trim, Post Trim).
2. 随机森林 (Random Forest Bagging).


剪枝
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
预剪枝: 指的是在生成树的时候剪. 比如我们可以限定树最大的高度, 每个叶节点包含的样本的最小个数, 每个叶节点最小的熵值.
后剪枝: 麻烦一点.


随机森林
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
