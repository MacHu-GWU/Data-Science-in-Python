Random Forest
==============================================================================

随机森林的参数:


讨论: 随机森林还可以用来做什么
------------------------------------------------------------------------------


使用随机森林建立计算样本间相似度 (similarity between samples)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
原理: 若两样本同时出现在相同叶节点的次数越多, 则两者越相似.

算法过程:

1. 记样本个数为N, 初始化一个N x N的零矩阵.
2. 我们想算1号样本与其他样本的相似度, 对于2号样本, 如果2号样本和1号样本出现在同一个叶节点k次, 那么我们设置M12设为K.
3. 持续这样做, 我们可以得到一个N x N的矩阵.
4. 将所有随机森林得出的相似度矩阵相加, 得到的就是一个相似度矩阵.


使用随机森林计算特征重要度 (feature selection)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
原理: 如果某一个特征在分支时被选择的次数比较多, 那么这个特征的重要性就比较大.

算法过程:

1. Selection Frequency (该特征被选择的次数).
2. Gini Importance (被选择时的基尼系数).
3. Permutation Importance (将该特征替换, 计算新模型正确率的变化).


异常检测 (outlier detection)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

原理: 若样本X为异常值, 它应该在大多数itree中很快从根达到叶子.

随机选择特征, 随机选择分割点, 生成 **一定深度** 的决策树iTree, 若干棵iTree组成了iForest (Isolation Forest).

- 计算iTree中样本x从根到叶子的长度f(x).
- 计算iForest中所有长度的综合F(x)

我们通常要求样本集的数量不能太多, 不然的话很多不是异常值的样本都会由于随机性, 被当成异常值. 所以我们在样本数量多的时候, 往往会先做降采样.
